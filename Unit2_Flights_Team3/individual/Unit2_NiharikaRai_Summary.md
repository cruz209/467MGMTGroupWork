I learned about the entire process of building an ops-ready classifier in BigQuery ML to predict the diverted flights in the US. The process began with standardizing data from a raw source (downloading from Kaggle and loading it into BigQuery via the GCS Bucket and renaming some columns for proper analysis), which was crucial for model training and consistency. Then, a simple logistic model was successfully trained using basic pre-flight features (dep_delay, distance, carrier, origin, dest, day_of_week) to provide a starting point for performance comparison. This was the baseline model that I created. The TRANSFORM clause proved effective for creating new, potentially more informative features directly within the model definition. Then, BigQuery ML's ML.EVALUATE provided standard metrics, while custom SQL queries generated confusion matrices at different probability thresholds, offering granular insight into True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). However, the dataset exhibited a significant imbalance, with very few diverted flights compared to non-diverted flights. This imbalance heavily impacted the models' ability to correctly identify positive cases (diversions).

Both the baseline and engineered models exhibited significant limitations, primarily due to the extreme class imbalance. The most critical failure was the consistently very low (or zero) recall for the 'diverted' class across both models and various thresholds. This means the models failed to identify the vast majority of actual diverted flights. At both the default 0.5 threshold and the custom 0.75 threshold, the baseline model predicted zero True Positives (TP=0) and zero False Positives (FP=0). This indicates it effectively classified almost all flights as non-diverted, resulting in a high number of False Negatives, making it nearly useless for proactive diversion prediction. While the engineered model showed an improved ROC AUC (0.7663 vs. 0.713678 for baseline), its recall for diverted flights remained very low (stated as 0.002006). Although it achieved perfect precision (1.0) for the few diversions it did predict, missing ~99.8% of actual diversions is a major operational shortcoming. A model with such low recall for the positive class is not suitable for deployment in scenarios where missing an actual event (a False Negative) has severe consequences (e.g., safety, preparedness, passenger experience). It establishes a 'no-real-time baseline' but highlights the need for more sophisticated techniques to handle imbalanced data or richer features. 

I would deploy the model with a custom prediction threshold of 0.75. This choice prioritizes precision over recall in an operational context. The primary justification is to minimize False Positives (FP), which represent instances where the model incorrectly predicts a flight will be diverted when it will not. This is because in flight operations, a false positive can lead to unnecessary resource allocation, operational inefficiencies, and wasted costs. These are all the reasons that our model would want to avoid.  


